# ğŸ§¹ ReFocused AI - Enterprise Dataset Processor

**Transform massive, messy datasets into high-quality training data for AI models**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## ğŸ¯ What It Does

ReFocused AI automatically processes and cleans large-scale datasets from multiple sources:

- **Reddit Data**: Cleans compressed JSONL files (tested up to 13GB)
- **HuggingFace Datasets**: Streams and processes datasets like OpenWebText (41GB)
- **Multi-Source Processing**: Handles up to 150GB total capacity
- **Quality Filtering**: Advanced deduplication, spam removal, and content scoring
- **Training Ready**: Outputs clean data in training-ready format

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- 8GB+ RAM recommended
- 50GB+ free disk space

### Installation
```bash
git clone <repository-url>
cd ReFocused-Ai
pip install -r requirements.txt
```

### Basic Usage

**1. Validate your system:**
```bash
python validate_setup.py
```

**2. Process your data:**
```bash
# For Reddit data only
python setup_massive_dataset.py --reddit /path/to/your/data.gz --extract
python process_massive_dataset.py

# For HuggingFace + Reddit (recommended)
python setup_massive_dataset.py --reddit /path/to/your/data.gz --huggingface --extract
python process_massive_dataset.py
```

**3. Monitor progress:**
```bash
tail -f logs/massive_processing.log
```

## ğŸ“Š What You Get

### Input â†’ Output
- **13GB Reddit data** â†’ **3-6GB cleaned posts** (65% retention)
- **41GB HuggingFace** â†’ **12-21GB cleaned text** (70% retention)  
- **Combined 54GB** â†’ **25-35GB training data** (7M+ high-quality samples)

### Processing Time (typical ranges)
| System Specs | Processing Time | Output Quality |
|--------------|----------------|----------------|
| 8GB RAM, 4 cores | 24â€“48 hours | 60â€“80% retention |
| 16GB RAM, 8 cores | 14â€“30 hours | 60â€“80% retention |
| 32GB RAM, 12 cores | 8â€“18 hours | 60â€“80% retention |

## ğŸ§¹ Cleaning Features

- **Smart Deduplication**: Content-based duplicate detection across sources
- **Quality Scoring**: Multi-criteria assessment (length, engagement, coherence)
- **Spam Filtering**: Pattern detection and profanity filtering
- **Format Unification**: Converts all sources to consistent JSONL
- **Memory Efficient**: Processes datasets larger than available RAM

## ğŸ“ Project Structure

```
ReFocused-AI/
â”œâ”€â”€ setup_massive_dataset.py    # Multi-source data extraction
â”œâ”€â”€ process_massive_dataset.py  # Optimized cleaning pipeline
â”œâ”€â”€ data_cleaner.py             # Core cleaning engine
â”œâ”€â”€ data_processor.py           # Training data preparation
â”œâ”€â”€ validate_setup.py           # System validation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ unified_raw/            # Extracted chunks
â”‚   â”œâ”€â”€ cleaned/                # Processed output
â”‚   â””â”€â”€ processed/              # Training splits
â””â”€â”€ logs/                       # Processing logs
```

## âš™ï¸ Configuration

### Memory Optimization
Edit `process_massive_dataset.py` to adjust for your system:

```python
# For limited RAM (8GB)
cleaner = RedditDataCleaner(n_workers=2)
cleaned_posts = cleaner.run_cleaning_pipeline(batch_size=2000)

# For high-end systems (32GB+)
cleaner = RedditDataCleaner(n_workers=12)
cleaned_posts = cleaner.run_cleaning_pipeline(batch_size=15000)
```

### Custom HuggingFace Datasets
```bash
python setup_massive_dataset.py --hf-dataset "allenai/c4" --huggingface --extract
```

## ğŸ”§ Advanced Usage

### Processing Pipeline
1. **Analysis**: Understand dataset size and format
2. **Extraction**: Convert to unified format in chunks
3. **Cleaning**: Apply quality filters and deduplication
4. **Processing**: Create training/validation/test splits

### Monitoring & Troubleshooting

**Real-time monitoring:**
```bash
# Progress
tail -f logs/massive_processing.log

# System resources  
htop -p $(pgrep -f process_massive_dataset)

# Disk usage
du -sh data/*
```

**Common issues:**
- **Out of memory**: Reduce batch size and workers
- **Slow processing**: Check CPU/disk bottlenecks with `htop`/`iotop`
- **Disk space**: Clean temporary files in `data/unified_raw/`

## ğŸ¯ Next Steps

After cleaning, prepare your data for training:

```bash
python data_processor.py      # Create train/val/test splits
python training_prep.py       # Setup training environment
python train_model.py         # Train your model
```

## ğŸ› ï¸ Technical Details

**Built with enterprise-scale processing in mind:**
- **Streaming Processing**: Handles larger-than-memory datasets
- **Parallel Architecture**: Multi-worker processing for speed
- **Progress Tracking**: Real-time statistics and comprehensive logging
- **Error Recovery**: Robust handling and processing resumption
- **Extensible Design**: Easy to add new data sources

**Based on proven data processing patterns:**
- [Automated Data Cleaning Best Practices](https://medium.com/@abhishekshaw020/how-to-automate-data-cleaning-for-large-datasets-b9d5a3236270)
- [Python Data Pipeline Architecture](https://www.quanthub.com/guide-for-using-python-for-data-extraction-in-a-data-pipeline/)

## ğŸ“ˆ Performance

**Tested at scale:**
- âœ… 150GB processing capacity
- âœ… 10M+ records processed
- âœ… 65-80% quality retention
- âœ… Memory-efficient streaming
- âœ… Multi-source deduplication

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Issues**: Report bugs or request features via GitHub Issues
- **Documentation**: See `DATASET_PROCESSING_GUIDE.md` for detailed workflows
- **System Check**: Run `python validate_setup.py` for system-specific guidance

---

**Ready to transform your messy data into training-ready datasets?** ğŸš€

Start with `python validate_setup.py` to check your system capabilities, then follow the Quick Start guide above. 