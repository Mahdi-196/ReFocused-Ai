# H100 Multi-GPU Training Configuration

# Model Configuration
model:
  type: gpt_neox
  config_path: "../models/gpt_750m/config.json"
  tokenizer_path: "../models/tokenizer/tokenizer"
  
# Training Configuration
training:
  output_dir: "/home/ubuntu/training_data/checkpoints"
  data_dir: "/home/ubuntu/training_data/shards"
  log_dir: "/home/ubuntu/training_data/logs"
  
  # Training parameters
  batch_size: 32
  gradient_accumulation_steps: 1
  total_steps: 100000
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  
  # Optimization
  fp16: true
  bf16: false
  gradient_checkpointing: true
  
  # Checkpointing
  save_steps: 500
  eval_steps: 500
  
  # Logging
  logging_steps: 10
  log_level: "info"
  wandb_project: "refocused-ai"
  
# DeepSpeed Configuration
deepspeed:
  config_path: "config/h100_deepspeed_multi.json"
  
# Data Management
data:
  bucket_name: "refocused-ai"
  remote_data_path: ""
  checkpoint_bucket_path: "Checkpoints"
  
# Monitoring & Logging
monitoring:
  wandb_project: "refocused-ai-training"
  wandb_entity: null
  report_to: ["wandb", "tensorboard"]
  logging_dir: "/home/ubuntu/training_data/logs"
  
  # Performance monitoring
  monitor_gpu_memory: false
  monitor_system_metrics: false
  log_gpu_metrics_interval: 60
  
# Hardware Configuration
hardware:
  num_gpus: 8
  gpu_type: "H100_SXM"
  node_type: "single_node"
  
# Tokenizer
tokenizer:
  path: "../models/tokenizer/tokenizer"
  
# Environment
environment:
  seed: 42
  cuda_visible_devices: "0,1,2,3,4,5,6,7"
  rdma_fork_safe: true 