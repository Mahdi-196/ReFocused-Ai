#!/usr/bin/env python3
"""
Hyperbolic Labs Setup Analysis
Real specifications and cost comparison
"""

def analyze_hyperbolic_setup():
    print("ğŸš€ HYPERBOLIC LABS 8x H100 SXM ANALYSIS")
    print("=" * 60)
    
    # Your actual specs
    gpus = 8
    gpu_vram_gb = 80
    total_gpu_vram = gpus * gpu_vram_gb
    system_ram_gb = 2048  # 2TB = 2048GB
    storage_tb = 2.3
    storage_gb = storage_tb * 1024
    cost_per_hour = 7.92
    
    print(f"ğŸ“Š YOUR ACTUAL SETUP:")
    print(f"   GPUs: {gpus}x H100 SXM")
    print(f"   GPU Memory: {gpu_vram_gb}GB Ã— {gpus} = {total_gpu_vram}GB total")
    print(f"   System RAM: {system_ram_gb:,}GB ({system_ram_gb/1024:.1f}TB)")
    print(f"   Storage: {storage_tb}TB ({storage_gb:,.0f}GB)")
    print(f"   CPU: Intel Xeon Platinum 8480")
    print(f"   Cost: ${cost_per_hour}/hour")
    
    print(f"\nğŸ’° COST COMPARISON WITH MARKET:")
    print(f"   Provider              Config             Price/Hour    8hr Cost")
    print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    
    # Market comparisons from getdeploying.com data
    competitors = [
        ("Hyperstack", "1x H100", 1.90, 1),
        ("Lambda Labs", "1x H100", 2.49, 1), 
        ("RunPod", "1x H100", 2.39, 1),
        ("FluidStack", "1x H100", 2.89, 1),
        ("GCP", "8x H100", "On Request", 8),
        ("AWS", "8x H100", "~$30-40", 8),
    ]
    
    print(f"   {'Hyperbolic (YOU)':<17} {'8x H100 SXM':<16} ${cost_per_hour:<10} ${cost_per_hour*8:<8}")
    
    for provider, config, price, gpu_count in competitors:
        if isinstance(price, str):
            if "~" in price:
                estimated_8_gpu = price
                eight_hr = "~$240-320"
            else:
                estimated_8_gpu = price
                eight_hr = price
        else:
            if gpu_count == 1:
                estimated_8_gpu = f"~${price * 8:.2f}"
                eight_hr = f"${price * 8 * 8:.0f}"
            else:
                estimated_8_gpu = f"${price:.2f}"
                eight_hr = f"${price * 8:.0f}"
        
        print(f"   {provider:<17} {config:<16} {estimated_8_gpu:<10} {eight_hr:<8}")
    
    print(f"\nğŸ¯ WHY YOUR DEAL IS INCREDIBLE:")
    
    # Calculate what competitors would charge
    single_h100_avg = 2.49  # Lambda Labs price as reference
    estimated_competitor_8x = single_h100_avg * 8
    your_savings_per_hour = estimated_competitor_8x - cost_per_hour
    training_hours = 40
    total_savings = your_savings_per_hour * training_hours
    
    print(f"   Single H100 average: ~${single_h100_avg}/hour")
    print(f"   8x H100 estimated: ~${estimated_competitor_8x}/hour")
    print(f"   Your price: ${cost_per_hour}/hour")
    print(f"   Savings per hour: ${your_savings_per_hour}/hour")
    print(f"   Total training savings: ${total_savings:.0f} (vs competitors)")
    
    print(f"\nâš¡ PERFORMANCE ADVANTAGES:")
    print(f"   ğŸ”¥ H100 SXM vs PCIe:")
    print(f"      â€¢ SXM: 3.35TB/s memory bandwidth")
    print(f"      â€¢ PCIe: 2TB/s memory bandwidth") 
    print(f"      â€¢ SXM is 67% faster memory access!")
    
    print(f"\n   ğŸ’¾ Massive Memory Configuration:")
    print(f"      â€¢ GPU Memory: {total_gpu_vram}GB (enough for 5x your model)")
    print(f"      â€¢ System RAM: {system_ram_gb/1024:.1f}TB (unlimited CPU offloading)")
    print(f"      â€¢ Storage: {storage_tb}TB NVMe (blazing fast I/O)")
    
    print(f"\n   ğŸš€ Training Optimizations Possible:")
    print(f"      â€¢ Zero memory pressure (640GB >> 3GB model)")
    print(f"      â€¢ Massive batch sizes possible")
    print(f"      â€¢ Full model + optimizer in GPU memory")
    print(f"      â€¢ Could train multiple models simultaneously")
    
    print(f"\nğŸ“ˆ REALISTIC PERFORMANCE ESTIMATE:")
    baseline_steps_per_hour = 2500
    sxm_boost = 1.15  # 15% boost from SXM vs PCIe
    memory_boost = 1.10  # 10% boost from abundant memory
    total_boost = sxm_boost * memory_boost
    optimized_steps_per_hour = baseline_steps_per_hour * total_boost
    
    print(f"   Baseline (PCIe H100): {baseline_steps_per_hour} steps/hour")
    print(f"   SXM memory boost: +{(sxm_boost-1)*100:.0f}%")
    print(f"   Abundant RAM boost: +{(memory_boost-1)*100:.0f}%")
    print(f"   Your expected speed: {optimized_steps_per_hour:.0f} steps/hour")
    
    # Recalculate costs with optimized performance
    target_steps = 100000
    optimized_hours = target_steps / optimized_steps_per_hour
    optimized_cost = optimized_hours * cost_per_hour
    
    print(f"\nğŸ’µ UPDATED COST ESTIMATE:")
    print(f"   Training time: {optimized_hours:.1f} hours (vs 40h baseline)")
    print(f"   Total cost: ${optimized_cost:.2f}")
    print(f"   Cost per million tokens: ${(optimized_cost/21_700_000_000)*1_000_000:.4f}")
    
    print(f"\nğŸ† BOTTOM LINE:")
    print(f"   You got an ENTERPRISE-GRADE setup for ${cost_per_hour}/hour")
    print(f"   This would cost $15-20/hour on AWS/GCP")
    print(f"   You're saving ~${(estimated_competitor_8x - cost_per_hour) * optimized_hours:.0f} total!")
    print(f"   Expected training cost: ${optimized_cost:.0f} (potentially even less!)")

if __name__ == "__main__":
    analyze_hyperbolic_setup() 