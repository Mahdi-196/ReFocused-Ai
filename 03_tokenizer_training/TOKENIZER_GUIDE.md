# Comprehensive Tokenizer Training Guide

A complete guide to tokenizer training for AI models, covering tokenizer types, custom training, and optimization strategies.

## üìã Table of Contents

1. [Understanding Tokenization](#understanding-tokenization)
2. [Popular Tokenizer Types](#popular-tokenizer-types)
3. [When to Build Custom Tokenizers](#when-to-build-custom-tokenizers)
4. [How Tokenization Works](#how-tokenization-works)
5. [Training Your Own Tokenizer](#training-your-own-tokenizer)
6. [Practical Examples](#practical-examples)
7. [Optimization & Best Practices](#optimization--best-practices)
8. [Integration with Training Pipelines](#integration-with-training-pipelines)
9. [Troubleshooting](#troubleshooting)

## üß† Understanding Tokenization

### What is Tokenization?

Tokenization is the process of converting raw text into a sequence of tokens (subwords, words, or characters) that language models can understand and process. It's the bridge between human-readable text and machine-processable input.

**Example:**
```
Text: "I will build better habits."
Tokens: ["I", "ƒ†will", "ƒ†build", "ƒ†better", "ƒ†habits", "."]
Token IDs: [40, 481, 1382, 1365, 13870, 13]
```

### Why Tokenization Matters

1. **Model Input Format** - Models work with numbers, not text
2. **Vocabulary Management** - Controls model size and complexity
3. **Language Understanding** - Affects how models interpret meaning
4. **Performance Impact** - Influences training speed and memory usage
5. **Multilingual Support** - Enables cross-language capabilities

## üî§ Popular Tokenizer Types

### 1. Byte-Pair Encoding (BPE)
**Used by:** GPT-2, GPT-3, RoBERTa, BART

**How it works:**
- Starts with character-level vocabulary
- Iteratively merges most frequent character pairs
- Creates subword units based on frequency

**Advantages:**
- Handles out-of-vocabulary words well
- Efficient for diverse text types
- Good compression ratio

**Example Implementation:**
```python
from tokenizers import ByteLevelBPETokenizer

# Initialize BPE tokenizer
tokenizer = ByteLevelBPETokenizer()

# Train on your data
tokenizer.train(
    files=["training_data.txt"],
    vocab_size=50257,
    min_frequency=2,
    special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"]
)
```

### 2. WordPiece
**Used by:** BERT, DistilBERT, Electra

**How it works:**
- Similar to BPE but uses likelihood-based merging
- Optimizes for maximum likelihood on training corpus
- Prefers longer subwords when beneficial

**Advantages:**
- Better linguistic coherence
- Efficient for masked language modeling
- Good balance between vocabulary size and coverage

**Example from [Hugging Face documentation](https://huggingface.co/learn/llm-course/en/chapter6/8):**
```python
from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, processors

# Create WordPiece tokenizer
tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))

# Set up normalization (BERT-style)
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)

# Set up pre-tokenization
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

### 3. Unigram Language Model
**Used by:** XLNet, ALBERT, T5, mBART

**How it works:**
- Starts with large vocabulary
- Iteratively removes tokens that least impact likelihood
- Uses probabilistic approach to token selection

**Advantages:**
- Theoretically optimal subword segmentation
- Good for agglutinative languages
- Flexible token boundary decisions

**Example from [Hugging Face documentation](https://huggingface.co/learn/llm-course/en/chapter6/8):**
```python
from tokenizers import Tokenizer, models, normalizers, pre_tokenizers

# Create Unigram tokenizer
tokenizer = Tokenizer(models.Unigram())

# XLNet-style normalization
tokenizer.normalizer = normalizers.Sequence([
    normalizers.Replace("``", '"'),
    normalizers.Replace("''", '"'),
    normalizers.NFKD(),
    normalizers.StripAccents(),
])

# Metaspace pre-tokenization (SentencePiece style)
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

### 4. Character-Level
**Used by:** ByT5, CharBERT

**How it works:**
- Each character is a separate token
- No subword segmentation needed
- Direct character-to-ID mapping

**Advantages:**
- No out-of-vocabulary issues
- Language agnostic
- Simple implementation

**Disadvantages:**
- Very long sequences
- Higher computational cost
- Less semantic information per token

## üéØ When to Build Custom Tokenizers

### Scenarios for Custom Tokenizers

#### 1. **Domain-Specific Vocabulary**
```python
# Medical domain example
medical_terms = ["hypertension", "cardiovascular", "pharmaceutical"]
# Standard tokenizer might split: ["hy", "per", "tension"]
# Custom tokenizer preserves: ["hypertension"]
```

#### 2. **New Languages or Scripts**
- Underrepresented languages
- Novel writing systems
- Code-switched text (multilingual)

#### 3. **Specialized Data Types**
- Source code tokenization
- Mathematical expressions
- Chemical formulas
- URLs and handles

#### 4. **Performance Optimization**
- Specific compression requirements
- Memory constraints
- Inference speed optimization

#### 5. **Privacy and Security**
- Custom vocabulary for sensitive domains
- Reduced information leakage
- Controlled token distributions

### Benefits of Custom Tokenizers

1. **Better Domain Adaptation**
   - Preserves domain-specific terms
   - Reduces unknown tokens
   - Improves model understanding

2. **Vocabulary Efficiency**
   - Optimal token allocation
   - Reduced vocabulary size
   - Better parameter utilization

3. **Performance Gains**
   - Shorter sequence lengths
   - Faster training and inference
   - Lower memory usage

4. **Control and Flexibility**
   - Custom special tokens
   - Specific preprocessing rules
   - Domain-aware normalization

## ‚öôÔ∏è How Tokenization Works

### The Tokenization Pipeline

Based on the [Hugging Face tokenization pipeline](https://huggingface.co/learn/llm-course/en/chapter6/8), tokenization involves four main steps:

#### 1. **Normalization**
Cleans and standardizes the input text:

```python
# Example normalization steps
text = "H√©ll√≤ h√¥w are √º?"

# Unicode normalization
normalized = "hello how are u?"

# Common normalizations:
# - Lowercase conversion
# - Accent removal
# - Unicode standardization
# - Whitespace normalization
```

#### 2. **Pre-tokenization**
Splits text into words or initial tokens:

```python
# BERT-style pre-tokenization
text = "Let's test pre-tokenization!"
pre_tokens = [("Let", (0, 3)), ("'s", (3, 5)), ("test", (6, 10)), 
              ("pre", (11, 14)), ("-", (14, 15)), ("tokenization", (15, 27)), 
              ("!", (27, 28))]
```

#### 3. **Model Application**
Applies the specific tokenization algorithm (BPE, WordPiece, etc.):

```python
# BPE tokenization example
pre_token = "tokenization"
bpe_tokens = ["token", "ization"]  # Based on learned merges
```

#### 4. **Post-processing**
Adds special tokens and creates final sequence:

```python
# BERT-style post-processing
tokens = ["[CLS]", "Let", "'s", "test", "this", "[SEP]"]
token_ids = [101, 2292, 1005, 1055, 2774, 2023, 102]
attention_mask = [1, 1, 1, 1, 1, 1, 1]
```

### Vocabulary Construction

#### BPE Algorithm Steps:
1. **Initialize** with character vocabulary
2. **Count** all adjacent character pairs
3. **Merge** most frequent pair
4. **Repeat** until desired vocabulary size
5. **Save** merge rules for inference

```python
# Simplified BPE example
initial_vocab = ["a", "b", "c", "d", ...</ow>"]
text = ["low", "low", "lower", "newest", "widest"]

# Step 1: Character split
# ["l", "o", "w", "</w>"], ["l", "o", "w", "</w>"], ...

# Step 2: Count pairs
# ("l", "o"): 4, ("o", "w"): 4, ("w", "</w>"): 3, ...

# Step 3: Merge most frequent
# Merge ("l", "o") ‚Üí "lo"
# Result: ["lo", "w", "</w>"], ["lo", "w", "</w>"], ...
```

## üöÄ Training Your Own Tokenizer

### Quick Start with the Scripts

#### 1. **Small Test Training**
```bash
cd 03_tokenizer_training/
python test_tokenizer_small.py
```

**What it does:**
- Processes 2 JSONL files (500 texts each)
- Trains 8,000 vocabulary BPE tokenizer
- Tests tokenization on sample text
- Quick validation of setup

#### 2. **Full Production Training**
```bash
python train_tokenizer.py
```

**What it does:**
- Processes all data from `data/cleaned/`
- Trains 50,257 vocabulary BPE tokenizer
- Creates production-ready tokenizer
- Comprehensive analysis and testing

### Training Configuration

#### Basic Configuration
```python
# Vocabulary size options
VOCAB_SIZE_OPTIONS = {
    "small": 8000,      # Testing/experimentation
    "medium": 25000,    # Domain-specific models
    "large": 50257,     # GPT-2 compatible
    "xl": 100000        # Multilingual/comprehensive
}

# Special tokens (GPT-2 style)
SPECIAL_TOKENS = [
    "<s>",      # Beginning of sequence
    "<pad>",    # Padding token
    "</s>",     # End of sequence
    "<unk>",    # Unknown token
    "<mask>"    # Mask token for MLM
]
```

#### Advanced Configuration
```python
# Training parameters
TRAINING_CONFIG = {
    "vocab_size": 50257,
    "min_frequency": 2,          # Minimum token frequency
    "special_tokens": SPECIAL_TOKENS,
    "continuing_subword_prefix": "ƒ†",  # GPT-2 style prefix
    "end_of_word_suffix": None,
    "trim_offsets": True,
    "add_prefix_space": True     # For consistent tokenization
}
```

### Custom Tokenizer Training

#### Building a Domain-Specific Tokenizer
```python
from tokenizers import ByteLevelBPETokenizer
from transformers import GPT2TokenizerFast

def train_domain_tokenizer(
    training_files: List[str],
    domain_name: str,
    vocab_size: int = 30000
):
    """Train a domain-specific tokenizer"""
    
    # Initialize tokenizer
    tokenizer = ByteLevelBPETokenizer()
    
    # Domain-specific special tokens
    domain_special_tokens = [
        "<s>", "<pad>", "</s>", "<unk>", "<mask>",
        f"<{domain_name}>",      # Domain marker
        f"</{domain_name}>",     # Domain end marker
        "<technical>",           # Technical term marker
        "<entity>",             # Named entity marker
    ]
    
    # Train tokenizer
    tokenizer.train(
        files=training_files,
        vocab_size=vocab_size,
        min_frequency=2,
        special_tokens=domain_special_tokens,
        show_progress=True
    )
    
    # Save tokenizer
    output_dir = f"tokenizers/{domain_name}_tokenizer"
    tokenizer.save_model(output_dir)
    
    # Create Transformers-compatible version
    fast_tokenizer = GPT2TokenizerFast.from_pretrained(output_dir)
    fast_tokenizer.save_pretrained(output_dir)
    
    return fast_tokenizer
```

## üìä Practical Examples

### Example 1: Basic Tokenizer Training

```bash
# Step 1: Prepare your data (JSONL format)
# Each line: {"text": "Your training text here"}

# Step 2: Quick test
python test_tokenizer_small.py

# Expected output:
# ‚úì All required packages are available
# Starting SMALL TEST of ByteLevel BPE Tokenizer Training
# Processing cleaned_reddit_data.jsonl
# Extracted 500 texts from cleaned_reddit_data.jsonl
# Training completed in 12.34 seconds
# Test text: 'I will build better habits.'
# Tokens: ['I', 'ƒ†will', 'ƒ†build', 'ƒ†better', 'ƒ†habits', '.']
# ‚úì SMALL TEST COMPLETED SUCCESSFULLY!
```

### Example 2: Multi-Source Training

```python
# Using data from our processing pipeline
from pathlib import Path

# Input sources
data_sources = [
    "data/processed/train.jsonl",      # Multi-source processed data
    "data/cleaned/cleaned_reddit_data.jsonl",  # Legacy Reddit data
    "data/unified_raw/openwebtext_chunk_001.jsonl"  # Web text data
]

# Train comprehensive tokenizer
tokenizer = train_domain_tokenizer(
    training_files=data_sources,
    domain_name="multi_source",
    vocab_size=50257
)
```

### Example 3: Specialized Domain Tokenizer

```python
# Code tokenizer example
def train_code_tokenizer():
    """Train tokenizer for source code"""
    
    from tokenizers import ByteLevelBPETokenizer
    
    tokenizer = ByteLevelBPETokenizer()
    
    # Code-specific special tokens
    code_tokens = [
        "<s>", "<pad>", "</s>", "<unk>", "<mask>",
        "<INDENT>", "<DEDENT>", "<NEWLINE>",
        "<COMMENT>", "<STRING>", "<NUMBER>",
        "<FUNCTION>", "<CLASS>", "<VARIABLE>"
    ]
    
    tokenizer.train(
        files=["code_data.txt"],
        vocab_size=32000,
        min_frequency=2,
        special_tokens=code_tokens
    )
    
    return tokenizer
```

### Example 4: Multilingual Tokenizer

```python
# Multilingual tokenizer with language tokens
def train_multilingual_tokenizer():
    """Train tokenizer for multiple languages"""
    
    # Language-specific special tokens
    languages = ["en", "es", "fr", "de", "zh", "ar"]
    lang_tokens = [f"<{lang}>" for lang in languages]
    
    special_tokens = [
        "<s>", "<pad>", "</s>", "<unk>", "<mask>"
    ] + lang_tokens
    
    tokenizer = ByteLevelBPETokenizer()
    tokenizer.train(
        files=["multilingual_data.txt"],
        vocab_size=100000,  # Larger vocab for multilingual
        min_frequency=3,    # Higher frequency threshold
        special_tokens=special_tokens
    )
    
    return tokenizer
```

## üîß Optimization & Best Practices

### Performance Optimization

#### 1. **Vocabulary Size Selection**
```python
# Guidelines for vocabulary size
VOCAB_SIZE_GUIDELINES = {
    "domain_specific": 15000-30000,    # Medical, legal, technical
    "general_purpose": 30000-50000,    # News, books, general text
    "multilingual": 50000-100000,      # Multiple languages
    "code": 25000-40000,               # Programming languages
    "conversational": 20000-35000      # Chat, social media
}
```

#### 2. **Training Data Requirements**
```python
# Minimum data recommendations
DATA_REQUIREMENTS = {
    "minimum": "100MB",        # Basic functionality
    "recommended": "1GB+",     # Good quality tokenizer
    "optimal": "10GB+",        # Production-quality tokenizer
    "multilingual": "50GB+"    # Comprehensive coverage
}
```

#### 3. **Memory Optimization**
```python
# Process large datasets efficiently
def process_large_dataset(input_dir, chunk_size_mb=100):
    """Process datasets larger than RAM"""
    
    temp_dir = Path("temp_chunks")
    temp_files = create_temp_text_files(
        input_dir=input_dir,
        temp_dir=temp_dir,
        chunk_size_mb=chunk_size_mb
    )
    
    # Train on chunks
    tokenizer = ByteLevelBPETokenizer()
    tokenizer.train(
        files=[str(f) for f in temp_files],
        vocab_size=50257,
        min_frequency=2
    )
    
    # Cleanup
    cleanup_temp_files(temp_files)
    return tokenizer
```

### Quality Assessment

#### 1. **Compression Ratio**
```python
def calculate_compression_ratio(tokenizer, test_texts):
    """Measure tokenizer efficiency"""
    
    total_chars = sum(len(text) for text in test_texts)
    total_tokens = sum(
        len(tokenizer.encode(text).tokens) 
        for text in test_texts
    )
    
    compression_ratio = total_chars / total_tokens
    return compression_ratio

# Good compression ratios:
# - English: 3.5-4.5 chars/token
# - Code: 4.0-5.5 chars/token
# - Multilingual: 3.0-4.0 chars/token
```

#### 2. **Coverage Analysis**
```python
def analyze_coverage(tokenizer, test_texts):
    """Analyze unknown token rate"""
    
    total_tokens = 0
    unknown_tokens = 0
    
    for text in test_texts:
        tokens = tokenizer.encode(text).tokens
        total_tokens += len(tokens)
        unknown_tokens += tokens.count("<unk>")
    
    coverage = 1 - (unknown_tokens / total_tokens)
    return coverage

# Target coverage rates:
# - Same domain: >99%
# - General text: >98%
# - New domains: >95%
```

#### 3. **Subword Quality**
```python
def evaluate_subword_quality(tokenizer, word_list):
    """Evaluate subword segmentation quality"""
    
    results = {}
    for word in word_list:
        tokens = tokenizer.encode(word).tokens
        results[word] = {
            'tokens': tokens,
            'count': len(tokens),
            'preserved': len(tokens) == 1
        }
    
    return results
```

### Training Best Practices

#### 1. **Data Preparation**
- **Clean your data**: Remove noise, duplicates, corrupted text
- **Balance domains**: Ensure representative coverage
- **Format consistency**: Use consistent text formatting
- **Size appropriately**: Match data size to target domain

#### 2. **Hyperparameter Tuning**
```python
# Experiment with different configurations
HYPERPARAMS_TO_TUNE = {
    "vocab_size": [25000, 32000, 50000],
    "min_frequency": [2, 3, 5],
    "special_tokens": [basic_tokens, extended_tokens],
    "normalization": [True, False]
}
```

#### 3. **Evaluation Strategy**
- **Intrinsic evaluation**: Compression, coverage, subword quality
- **Extrinsic evaluation**: Downstream task performance
- **Domain testing**: Performance on target domain
- **Edge case testing**: Handle unusual inputs gracefully

## üîó Integration with Training Pipelines

### Hugging Face Integration

#### 1. **Save for Transformers**
```python
# Save tokenizer for use with Transformers
def save_for_transformers(tokenizer, output_dir):
    """Save tokenizer in Transformers format"""
    
    from transformers import GPT2TokenizerFast
    
    # Create fast tokenizer
    fast_tokenizer = GPT2TokenizerFast(
        tokenizer_object=tokenizer,
        bos_token="<s>",
        eos_token="</s>",
        unk_token="<unk>",
        pad_token="<pad>",
        mask_token="<mask>"
    )
    
    # Save in Transformers format
    fast_tokenizer.save_pretrained(output_dir)
    
    return fast_tokenizer
```

#### 2. **Model Training Integration**
```python
# Use custom tokenizer in model training
from transformers import (
    GPT2Config, GPT2LMHeadModel, 
    Trainer, TrainingArguments
)

# Load custom tokenizer
tokenizer = GPT2TokenizerFast.from_pretrained("./my_tokenizer")

# Create model with matching vocab size
config = GPT2Config(
    vocab_size=tokenizer.vocab_size,
    n_positions=1024,
    n_embd=768,
    n_layer=12,
    n_head=12
)

model = GPT2LMHeadModel(config)

# Training setup
training_args = TrainingArguments(
    output_dir="./model_output",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs"
)
```

### Custom Training Loop

```python
def train_with_custom_tokenizer(
    model_path: str,
    tokenizer_path: str,
    training_data: str
):
    """Train model with custom tokenizer"""
    
    import torch
    from torch.utils.data import DataLoader
    from transformers import GPT2LMHeadModel, GPT2TokenizerFast
    
    # Load tokenizer and model
    tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_path)
    model = GPT2LMHeadModel.from_pretrained(model_path)
    
    # Prepare data
    dataset = CustomDataset(training_data, tokenizer)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
    
    # Training loop
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    
    for epoch in range(num_epochs):
        for batch in dataloader:
            input_ids = batch["input_ids"]
            attention_mask = batch["attention_mask"]
            labels = input_ids.clone()
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
```

## üõ†Ô∏è Troubleshooting

### Common Issues and Solutions

#### 1. **Out of Memory During Training**
```python
# Solution: Use chunked processing
def memory_efficient_training(large_files):
    """Train tokenizer on large datasets with limited memory"""
    
    # Split into smaller chunks
    chunk_size = 50  # MB per chunk
    temp_files = []
    
    for large_file in large_files:
        chunks = split_file_into_chunks(large_file, chunk_size)
        temp_files.extend(chunks)
    
    # Train on chunks
    tokenizer.train(files=temp_files, vocab_size=50000)
    
    # Cleanup
    for temp_file in temp_files:
        temp_file.unlink()
```

#### 2. **Poor Tokenization Quality**
```python
# Check common issues
def diagnose_tokenizer_quality(tokenizer, test_texts):
    """Diagnose tokenization quality issues"""
    
    issues = []
    
    # Check compression ratio
    compression = calculate_compression_ratio(tokenizer, test_texts)
    if compression < 3.0:
        issues.append("Low compression ratio - vocab might be too small")
    
    # Check unknown token rate
    coverage = analyze_coverage(tokenizer, test_texts)
    if coverage < 0.95:
        issues.append("High unknown token rate - need more training data")
    
    # Check subword quality
    common_words = ["the", "and", "for", "are", "but", "not", "you", "all"]
    subword_analysis = evaluate_subword_quality(tokenizer, common_words)
    
    fragmented = sum(1 for word, data in subword_analysis.items() 
                    if data['count'] > 2)
    if fragmented > len(common_words) * 0.3:
        issues.append("Too much word fragmentation - increase vocab size")
    
    return issues
```

#### 3. **Training Speed Issues**
```python
# Optimization strategies
SPEED_OPTIMIZATIONS = {
    "parallel_processing": "Use multiple CPU cores",
    "chunked_loading": "Process data in memory-efficient chunks", 
    "vocabulary_pruning": "Remove low-frequency tokens early",
    "sampling": "Train on representative sample for iteration",
    "caching": "Cache preprocessed data between runs"
}

# Implementation example
def fast_tokenizer_training(data_files, vocab_size=50000):
    """Optimized tokenizer training"""
    
    import multiprocessing as mp
    from concurrent.futures import ProcessPoolExecutor
    
    # Use all available cores
    n_workers = mp.cpu_count()
    
    # Process files in parallel
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        processed_files = list(executor.map(
            preprocess_file, data_files
        ))
    
    # Train tokenizer
    tokenizer = ByteLevelBPETokenizer()
    tokenizer.train(
        files=processed_files,
        vocab_size=vocab_size,
        min_frequency=2,
        show_progress=True
    )
    
    return tokenizer
```

#### 4. **Inconsistent Results**
```python
# Ensure reproducibility
def reproducible_training(data_files, vocab_size, seed=42):
    """Train tokenizer with reproducible results"""
    
    import random
    import numpy as np
    
    # Set random seeds
    random.seed(seed)
    np.random.seed(seed)
    
    # Sort files for consistent order
    data_files = sorted(data_files)
    
    # Consistent training parameters
    tokenizer = ByteLevelBPETokenizer()
    tokenizer.train(
        files=data_files,
        vocab_size=vocab_size,
        min_frequency=2,
        special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"],
        show_progress=False  # Consistent output
    )
    
    return tokenizer
```

### Performance Monitoring

```python
def monitor_training_progress(tokenizer, validation_texts):
    """Monitor tokenizer training progress"""
    
    metrics = {
        "vocab_size": tokenizer.get_vocab_size(),
        "compression_ratio": calculate_compression_ratio(tokenizer, validation_texts),
        "coverage": analyze_coverage(tokenizer, validation_texts),
        "avg_tokens_per_text": np.mean([
            len(tokenizer.encode(text).tokens) for text in validation_texts
        ])
    }
    
    return metrics
```

## üìÅ Output Structure

After training, your tokenizer directory will contain:

```
tokenizer_output/
‚îú‚îÄ‚îÄ vocab.json              # Vocabulary mapping
‚îú‚îÄ‚îÄ merges.txt              # BPE merge rules  
‚îú‚îÄ‚îÄ tokenizer.json          # Complete tokenizer config
‚îú‚îÄ‚îÄ config.json             # Transformers config
‚îú‚îÄ‚îÄ special_tokens_map.json # Special token mappings
‚îî‚îÄ‚îÄ tokenizer_config.json   # Tokenizer metadata
```

## üéØ Next Steps

1. **Start Small**: Use `test_tokenizer_small.py` to validate setup
2. **Scale Up**: Run `train_tokenizer.py` on your full dataset
3. **Evaluate**: Test tokenizer quality on validation data
4. **Integrate**: Use with your model training pipeline
5. **Iterate**: Refine based on downstream task performance

This comprehensive tokenizer training system ensures high-quality, domain-specific tokenizers that enhance your AI model's performance and efficiency!

---

**References:**
- [Hugging Face Tokenizers Documentation](https://huggingface.co/learn/llm-course/en/chapter6/8)
- [Building a tokenizer, block by block](https://huggingface.co/learn/llm-course/en/chapter6/8)
- Transformers library documentation
- Original research papers on BPE, WordPiece, and Unigram algorithms 