# ğŸš€ ReFocused-AI: 1.2B Parameter Language Model

Complete training system for the ReFocused-AI language model with Google Cloud Storage integration and authenticated checkpoint uploading.

## ğŸŒ± Why this project exists

I built this to power my ReFocused application. It generates useful, personalized content like recommendations, interesting facts, weekly themes, and it supports chatting with the model. I also wanted to see if I could build a full endâ€‘toâ€‘end training stack after reading projects like llm.c â€” the lowâ€‘level details were fascinating and I wanted to learn by doing.

## ğŸ“š What I learned

I ended up going a lot deeper than I planned, mostly because the internals kept pulling me in even after everything â€œworked.â€ Getting from raw text to tokens to attention blocks made me appreciate how many tiny choices (padding, masks, precision) quietly decide whether training feels smooth or fragile. I also built a practical feel for the whole pipeline: data quality matters most, stability and checkpointing arenâ€™t optional, and scaling only helps if your I/O keeps up.

Iâ€™m still kind of amazed that these models stay coherent and are often accurate in normal use â€” and Iâ€™ve also seen enough failure cases to respect their limits. On the engineering side, the unglamorous parts carried a lot of weight: sharding, streaming, background uploads, and â€œresume actually resumes.â€ The performance wins that consistently helped were simple: mixed precision, `torch.compile` where it works, and sensible dataloader settings.

## ğŸ’› Favroite feature

my favorite things about this pipeline is it really is user friendly it give steps tells you what worked and what didnt and how to fix it all in the command line another thing for me is i hate emojis in applications i think its a lower quality look but in this theyre spammed throughtout just to give that terminal some color plus seeing the rocket emoji after fialing over and over or the green checkmark after a bunch of red x's is enough to bring a tear down your face.

## ğŸ§± Pipeline overview (endâ€‘toâ€‘end)

1. `01_data_collection/`: Optional collectors for Reddit/Wikipedia; realâ€‘time monitoring tools.
2. `02_data_processing/`: Clean, dedupe, score quality, and create train/val/test splits.
3. `03_tokenizer_training/`: Train a ByteLevel BPE tokenizer (GPTâ€‘2 style) on your cleaned text.
4. `04_data_tokenization/`: Convert text into fixedâ€‘length token sequences (`.npz` with input_ids/masks).
5. `05_model_training/`: Train the 1.2B GPTâ€‘NeoX model; mixed precision, resume, and GCS checkpoints.
6. `07_utilities/`: Analysis scripts (dataset stats, tokenized data checks, quick counts).

## ğŸ“‹ Quick Start

**ğŸ‘‰ For complete setup instructions, see [SETUP_GUIDE.md](SETUP_GUIDE.md)**

### TL;DR Setup
```bash
# Clone and setup
git clone https://github.com/Mahdi-196/ReFocused-Ai.git
cd ReFocused-Ai
python -m venv refocused_env
source refocused_env/bin/activate  # Linux/Mac
# or refocused_env\Scripts\activate  # Windows
pip install -r requirements.txt

# Configure training
cd 05_model_training
# [Add your service account key to credentials/]
./start_training.sh --config test
```

## ğŸ—‚ï¸ Project Structure

```
ReFocused-Ai/
â”œâ”€â”€ 01_data_collection/         # Collect + monitor (Reddit/Wikipedia)
â”œâ”€â”€ 02_data_processing/         # Clean, qualityâ€‘score, splits
â”œâ”€â”€ 03_tokenizer_training/      # Train tokenizer (BPE)
â”œâ”€â”€ 04_data_tokenization/       # Produce .npz token shards
â”œâ”€â”€ 05_model_training/          # ğŸ¯ Training system (GPTâ€‘NeoX 1.2B)
â”‚   â”œâ”€â”€ train.py                # Main training script
â”‚   â”œâ”€â”€ start_training.sh       # Oneâ€‘click launcher
â”‚   â”œâ”€â”€ TRAINING_README.md      # Detailed training docs
â”‚   â”œâ”€â”€ configs/                # Model/training configs
â”‚   â”œâ”€â”€ utils/                  # Dataloaders, checkpoints, metrics
â”‚   â”œâ”€â”€ credentials/            # GCS keys (userâ€‘provided)
â”‚   â””â”€â”€ checkpoints/            # Local checkpoint storage
â”œâ”€â”€ 07_utilities/               # Analysis scripts
â”œâ”€â”€ SETUP_GUIDE.md              # ğŸ“– Complete setup guide
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## âœ¨ Key Features (whole pipeline)

- **Data collection**: Realâ€‘time Reddit/Wikipedia collectors with monitoring (optional)
- **Processing**: Dedup/quality filters, clean JSONL, balanced splits
- **Tokenizer**: Train BPE (GPTâ€‘2 style) and save Transformersâ€‘compatible artifacts
- **Tokenization**: Shard to `.npz` with input_ids/attention_mask ready for PyTorch
- **Training**: Mixed precision, deviceâ€‘aware `torch.compile`, resume, metrics
- **Checkpoints**: Background uploads to GCS with metadata; reliable resume
- **Monitoring**: CLI logs, TensorBoard support; progress and performance hints
- **Configs**: Test and production presets; easy overrides for steps/batch size

## ğŸš€ Endâ€‘toâ€‘end Quick Start

```bash
# 1) (Optional) Collect data
python 01_data_collection/Wikipedia-Collector.py  # or use existing data

# 2) Process & split
python 02_data_processing/data_cleaner.py
python 02_data_processing/data_processor.py

# 3) Train tokenizer (or skip if using an existing one)
python 03_tokenizer_training/train_tokenizer.py

# 4) Tokenize to .npz shards
python 04_data_tokenization/run_full_tokenization.py

# 5) Train the model
./start_training.sh --config test

# Production training
./start_training.sh --config production

# Custom steps
./start_training.sh --config test --max-steps 2000

# Resume from checkpoint
./start_training.sh --config test --resume checkpoint-name

# Monitor progress
tail -f logs/training.log
```

## ğŸ“Š Training Configurations

| Config | Steps | Files | Batch Size | Duration | Purpose |
|--------|-------|-------|------------|----------|---------|
| **test** | 1000 | 5 | 1 | ~10-30 min | Testing, experiments |
| **production** | 10000 | All | 4 | Hours-days | Full training |

## â˜ï¸ Checkpoint System

- **Automatic uploads** to `gs://refocused-ai/Checkpoints/`
- **Comprehensive metadata** with training metrics
- **Background processing** for non-blocking uploads
- **Resume capability** from any checkpoint
- **Local cleanup** of old checkpoints

## ğŸ”§ Requirements

- **Python 3.9+** (recommended: 3.11)
- **PyTorch 2.0+** with CUDA support (optional but recommended)
- **Google Cloud Storage** access with service account credentials
- **8GB+ RAM** (16GB+ recommended)
- **NVIDIA GPU** (optional but significantly faster)

## ğŸ“– Documentation

- **[SETUP_GUIDE.md](SETUP_GUIDE.md)**: Complete setup instructions with virtual environment
- **[05_model_training/TRAINING_README.md](05_model_training/TRAINING_README.md)**: Detailed training documentation
- **[configs/](05_model_training/configs/)**: Training and model configuration files

## ğŸ¯ Model Details

- **Architecture**: GPT-NeoX
- **Parameters**: ~1.2 billion
- **Context Length**: 2048 tokens  
- **Vocabulary**: 50,257 tokens
- **Training Data**: Reddit conversations (cleaned and tokenized)

## ğŸ¤ Contributing

1. Follow the setup guide to get training working
2. Make changes in appropriate directories
3. Test with `./start_training.sh --config test --max-steps 5`
4. Submit pull requests with clear descriptions

## ğŸ“ Support

For issues:
1. Check [SETUP_GUIDE.md](SETUP_GUIDE.md) troubleshooting section
2. Review training logs in `05_model_training/logs/`
3. Verify virtual environment and dependencies
4. Ensure credentials are properly configured

---

**Ready to train? Start with [SETUP_GUIDE.md](SETUP_GUIDE.md)! ğŸš€** 